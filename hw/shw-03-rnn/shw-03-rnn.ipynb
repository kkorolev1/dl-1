{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1c368f2",
   "metadata": {},
   "source": [
    "# Глубинное обучение 1 / Введение в глубинное обучение, ФКН ВШЭ\n",
    "\n",
    "## Домашнее задание 3: RNN и языковые модели \n",
    "\n",
    "### Общая информация\n",
    "\n",
    "Оценка после штрафа после мягкого дедлайна вычисляется по формуле $M_{\\text{penalty}} = M_{\\text{full}} \\cdot 0.85^{t/1440}$, где $M_{\\text{full}}$ — полная оценка за работу без учета штрафа, а $t$ — время в минутах, прошедшее после мягкого дедлайна (округление до двух цифр после запятой). Таким образом, спустя первые сутки после мягкого дедлайна вы не можете получить оценку выше 12.75, а если сдать через четыре дня после мягкого дедлайна, то ваш максимум — 7.83 балла.\n",
    "\n",
    "### Оценивание и штрафы\n",
    "\n",
    "Максимально допустимая оценка за работу — 15 баллов. Сдавать задание после указанного срока сдачи нельзя.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке. Также оценка может быть снижена за плохо читаемый код и плохо оформленные графики. Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n",
    "\n",
    "### О задании\n",
    "\n",
    "В этом задании вам предстоит обучить рекуррентную нейронную сеть для задачи генерации текстов. В качестве данных возьмем набор из 120 тысяч анекдотов (всех категорий от А до Я включительно). Его вы можете найти в архиве `jokes.txt.zip`, который доступен по [ссылке](https://disk.yandex.com/d/fjt5xICH-ukEEA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b35b1cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daae037",
   "metadata": {},
   "source": [
    "## Задание 1: Dataset (1 балл)\n",
    "\n",
    "В этом задании мы будет пользоваться библиотекой [sentencepiece](https://github.com/google/sentencepiece), которая поддерживает разные форматы токенизации текстов, в том числе BPE, который мы и будем использовать. Реализуйте недостающие фрагменты кода в классе `TextDataset` в файле `dataset.py`. Датасет обучает sentencepiece токенизатор, токенизирует тексты, превращает токены в индексы и паддит до одной и той же длины (параметр `max_length`). Не забудьте, что для генерации текстов нам будут нужны специальные токены начала и конца последовательности, соответственно `BOS` и `EOS`. Существуют еще два специальных токена &mdash; паддинг `PAD` и токен `UNK`, заменяющий out-of-vocabulary токены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dfa4648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.97\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c880c68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a0ed11c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from dataset import TextDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "81630e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TextDataset(data_file='jokes.txt', train=True, sp_model_prefix='bpe')\n",
    "valid_set = TextDataset(data_file='jokes.txt', train=False, sp_model_prefix='bpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "27555b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Код должен проходить тесты\n",
    "assert len(train_set) + len(valid_set) == 120759\n",
    "\n",
    "for _ in range(5):\n",
    "    for dataset in (train_set, valid_set):\n",
    "        indices, length = dataset[np.random.randint(len(dataset))]\n",
    "        assert indices.shape == (dataset.max_length, )\n",
    "        assert indices[0].item() == dataset.bos_id\n",
    "        assert (indices == dataset.eos_id).sum().item() == 1\n",
    "\n",
    "        eos_pos = indices.tolist().index(dataset.eos_id)\n",
    "        assert torch.all(indices[eos_pos + 1:] == dataset.pad_id)\n",
    "        assert (indices != dataset.pad_id).sum() == length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db087d5",
   "metadata": {},
   "source": [
    "## Задание 2 Language model (3.5 балла)\n",
    "\n",
    "Реализуйте класс `LanguageModel` из файла `model.py`. Мы будем генерировать текст с помощью языковой модели &mdash; это авторегрессионная вероятностная модель, которая предсказывает распределение следующего токена при условии предыдущих:\n",
    "\n",
    "$$\n",
    "p(x_1, x_2, x_3, \\dots, x_T) = p(x_1) \\cdot p(x_2 | x_1) \\cdot p(x_3|x_1, x_2) \\, \\cdot \\, \\dots \\, \\cdot \\, p(x_T|x_1, \\dots, x_{T-1})\n",
    "$$\n",
    "\n",
    "Мы будем реализовывать ее с помощью рекуррентной нейронной сети. Ваш код должен поддерживать возможность работать как с оригинальной [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN), так и c [LSTM](https://bitly.com/98K8eH). На каждом временном шаге модель возвращает логиты вероятностей для следующего токена. Модель будет работать в двух режимах (не путать с `.train()` и `.eval()`):\n",
    "\n",
    "- В режиме обучения (метод `forward`) модель принимает настоящие последовательности из датасета и их длины. На каждом временном шаге возвращаются логиты вероятностей следующего токена, что позволяет считать лосс, обучаться на трейне и валидироваться на валидации.\n",
    "\n",
    "- В режиме генерации (инференса, метод `inference`) модель принимает некоторый префикс (возможно пустой), с которого начинать генерацию, и продолжает его. Для этого на каждом шаге генерируются новые логиты, семплируется новый токен (из распределения, заданного логитами), и процесс продолжается, пока не будет сгенерирован токен `UNK` или не будет достигнуто ограничение на длину последовательности. **Обратите внимание**, что вам не нужно прогонять всю последовательность заново через RNN после каждого нового токена, это приведет к квадратичной сложности по длине последовательности. Вам достаточно обновлять скрытое состояние, подавая на вход новый сгенерированный токен и предыдущее скрытое состояние. Кроме того, чтобы получить больше контроля над генерацией, вводится параметр температуры `temp`. Перед семплированием нужно разделить на него логиты, полученные моделью. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "378c1231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import LanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c224633d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "99427388",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_model = LanguageModel(train_set, device=None).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "199406f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Код должен проходить тесты\n",
    "for bs in [1, 4, 16, 64, 256]:\n",
    "    indices = torch.randint(high=train_set.vocab_size, size=(bs, train_set.max_length)).to(device)\n",
    "    lengths = torch.randint(low=1, high=train_set.max_length + 1, size=(bs, ))\n",
    "    logits = lang_model(indices, lengths)\n",
    "    assert logits.shape == (bs, lengths.max(), train_set.vocab_size)\n",
    "\n",
    "for prefix in ['', 'купил мужик шляпу,', 'сел медведь в машину и', 'подумал штирлиц']:\n",
    "    generated = lang_model.inference(prefix, temp=np.random.uniform(0.1, 10))\n",
    "    assert type(generated) == str\n",
    "    assert generated.startswith(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eacf83",
   "metadata": {},
   "source": [
    "## Задание 3: Training (2 балла)\n",
    "\n",
    "Всё, что нам осталось &mdash; реализовать цикл обучения. Заполните пропуски в файле `train.py`. Не забудьте, что мы учим модель предсказывать вероятность следующего, а не текущего токена. Также рекомендуется обрезать батч индексов по самой длинной последовательности, чтобы не гонять паддинги вхолостую. Для оценки качества генерации будем использовать метрику [perplexity](https://towardsdatascience.com/perplexity-in-language-models-87a196019a94). Реализуйте ее подсчет в функции `plot_losses` (да, для этого достаточно только значения лосса).\n",
    "\n",
    "Обучите модель, используя ванильную RNN в качестве рекуррентного слоя. Сохраните чекпойнт обученной модели, он нам еще пригодится. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fd6ab7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿\n",
    "import torch.nn as nn\n",
    "from train import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9573cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "\n",
    "optimizer = torch.optim.Adam(lang_model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "num_epochs = 5\n",
    "num_examples = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3b2325ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6855d2080641f0b8b8eb9af17ea772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training 1/5:   0%|          | 0/28681 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(lang_model, optimizer, scheduler, train_loader, val_loader, num_epochs, num_examples)\n",
      "File \u001b[0;32m~/dl-1/hw/shw-03-rnn/train.py:134\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, train_loader, val_loader, num_epochs, num_examples)\u001b[0m\n\u001b[1;32m    131\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[39m=\u001b[39mtrain_loader\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mpad_id)\n\u001b[1;32m    133\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m--> 134\u001b[0m     train_loss \u001b[39m=\u001b[39m training_epoch(\n\u001b[1;32m    135\u001b[0m         model, optimizer, criterion, train_loader,\n\u001b[1;32m    136\u001b[0m         tqdm_desc\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mTraining \u001b[39;49m\u001b[39m{\u001b[39;49;00mepoch\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mnum_epochs\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m\n\u001b[1;32m    137\u001b[0m     )\n\u001b[1;32m    138\u001b[0m     val_loss \u001b[39m=\u001b[39m validation_epoch(\n\u001b[1;32m    139\u001b[0m         model, criterion, val_loader,\n\u001b[1;32m    140\u001b[0m         tqdm_desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValidating \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m scheduler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/dl-1/hw/shw-03-rnn/train.py:80\u001b[0m, in \u001b[0;36mtraining_epoch\u001b[0;34m(model, optimizer, criterion, loader, tqdm_desc)\u001b[0m\n\u001b[1;32m     77\u001b[0m     train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m indices\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     79\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 80\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     82\u001b[0m train_loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(loader\u001b[39m.\u001b[39mdataset)\n\u001b[1;32m     83\u001b[0m \u001b[39mreturn\u001b[39;00m train_loss\n",
      "File \u001b[0;32m~/.conda/envs/kkorolev/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/kkorolev/lib/python3.8/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.conda/envs/kkorolev/lib/python3.8/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/.conda/envs/kkorolev/lib/python3.8/site-packages/torch/optim/adam.py:178\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39m@_use_grad_for_differentiable\u001b[39m\n\u001b[1;32m    169\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, closure\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m, grad_scaler\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    170\u001b[0m     \u001b[39m\"\"\"Performs a single optimization step.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \n\u001b[1;32m    172\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39m            supplied from ``grad_scaler.step(optimizer)``.\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cuda_graph_capture_health_check()\n\u001b[1;32m    180\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/kkorolev/lib/python3.8/site-packages/torch/optim/optimizer.py:100\u001b[0m, in \u001b[0;36mOptimizer._cuda_graph_capture_health_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_graph_capture_health_check\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     99\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mhas_cuda \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m--> 100\u001b[0m         capturing \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mis_current_stream_capturing()\n\u001b[1;32m    102\u001b[0m         \u001b[39mif\u001b[39;00m capturing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mcapturable\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m    103\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAttempting CUDA graph capture of step() for an instance of \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\n\u001b[1;32m    104\u001b[0m                                \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m+\u001b[39m\n\u001b[1;32m    105\u001b[0m                                \u001b[39m\"\u001b[39m\u001b[39m but this instance was constructed with capturable=False.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/kkorolev/lib/python3.8/site-packages/torch/cuda/graphs.py:24\u001b[0m, in \u001b[0;36mis_current_stream_capturing\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_current_stream_capturing\u001b[39m():\n\u001b[1;32m     19\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m    Returns True if CUDA graph capture is underway on the current CUDA stream, False otherwise.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[39m    If a CUDA context does not exist on the current device, returns False without initializing the context.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m _cuda_isCurrentStreamCapturing()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "train(lang_model, optimizer, scheduler, train_loader, val_loader, num_epochs, num_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dad5e1",
   "metadata": {},
   "source": [
    "## Задание 4: LSTM (0.5 балла)\n",
    "\n",
    "Обучите аналогичную модель, но с LSTM в качестве рекуррентного слоя. Сравните модели по метрикам и генерации. Не забывайте про чекпойнты!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d457cd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d23819c",
   "metadata": {},
   "source": [
    "## Задание 5: Sampling temperature (0.5 балла)\n",
    "\n",
    "Поэкспериментируйте, как результат генерации зависит от параметра температуры. Попробуйте генерацию с разными префиксами. Сделайте выводы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bd1bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3967c7",
   "metadata": {},
   "source": [
    "## Задание 5: Tokenizers (1 балл)\n",
    "\n",
    "До сих пор мы использовали BPE токенизатор с относительно небольшим числом токенов (2000 по умолчанию). Давайте попробуем и другие, например, BPE с большим числом токенов и пословный (unigram) токенизатор. Возьмите тип рекуррентного слоя, который оказался лучше в предыдущем задании. Обучите модели на таких токенизаторах и сравните их генерацию. Не забывайте сохранять чекпойнты. Правильно ли сравнивать между собой получившиеся модели по значению perplexity? Почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4589b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc550928",
   "metadata": {},
   "source": [
    "## Задание 6. Latent Semantic Analysis (2 балла)\n",
    "\n",
    "Попробуем другой подход к оцениванию качества генерации, основанный на [Latent Semantic Analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis). Реализуйте следующую метрику и сравните по ней модели, обученные с разными токенизаторами:\n",
    "\n",
    "1. Генерируем обученной моделью выборку текстов, совпадающую по размеру с валидационной выборкой.\n",
    "2. Объединяем две выборки текстов (валидационную и сгенерированную) в один корпус. Обратите внимание, что наша токенизация в общем случае необратима, поэтому для чистоты эксперимента нужно закодировать и декодировать валидационную выборку.\n",
    "3. Генерируем tf-idf матрицу для полученного корпуса.\n",
    "4. Понижаем размерность матрицы с помощью [SVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html).\n",
    "5. Теперь у нас есть векторы, описывающие валидационные и сгенерированные тексты, лежащие в одном пространстве. Для каждого вектора, отвечающего сгенерированному тексту, найдем наибольший cosine similarity между ним и вектором валидационного текста. Усредним такие similarity по всем сгенерированным текстам и получим число, характеризующее похожесть сгенерированной выборки на валидационную.\n",
    "\n",
    "Какие плюсы и минусы есть у описанной метрики?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1415e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e632af",
   "metadata": {},
   "source": [
    "## Задание 7. Visualization (1 балл)\n",
    "\n",
    "В прошлом пункте мы получили векторы, описывающие валидационные и сгенерированные тексты. Попробуем визуализировать их. Примените [TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) к этим векторам и нарисуйте scatter-plot с получившимися двумерными представлениями. Точки, соответствующие валидационным и сгенерированным текстам, должны быть разного цвета. Визуализируйте таким образом все три модели для разных токенизаторов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aada1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d9cba5",
   "metadata": {},
   "source": [
    "## Задание 8. ruGPT perplexity (3.5 балла)\n",
    "\n",
    "Подход Latent Semantic Analysis, как и многие другие классические методы, заметно уступает нейросетевым алгоритмам анализа текстов. Вернемся к оцениванию качества генерации с помощью perplexity, для этого возьмем большую и хорошо обученную языковую модель, которая училась на огромном корпусе русских текстов. Считается, что большие языковые модели хорошо выучивают естественный язык, потому с их помощью мы сможем оценивать качество наших маленьких моделей для генерации анекдотов. Для этого мы воспользуемся сервисом [HuggingFace](https://huggingface.co/), который содержит огромное число обученных моделей для самых разных задач. Изучите и реализуйте, [подсчет perplexity](https://huggingface.co/docs/transformers/perplexity), с использованием обученной языковой модели. В качестве модели возьмите [ruGPT3-small](https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2). Сгенерируйте синтетические выборки тремя моделями, обученными выше (можете взять выборки из задания 6), и сравните их по perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cced66c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c5d3dc",
   "metadata": {},
   "source": [
    "## Бонус (0.1 балл)\n",
    "\n",
    "Покажите лучший анекдот, который удалось сгенерировать вашей модели. Если проверяющий найдет его смешным, то поставит 0.1 балла."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kkorolev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "f404e0e273a4f0796c7e39d3cee33e66306a58991d52ed6c42396fee84b35bae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
