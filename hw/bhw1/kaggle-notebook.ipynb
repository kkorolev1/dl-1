{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!g1.1\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as T\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision.models import resnet18, resnet50\nfrom torch.optim.swa_utils import AveragedModel\n\nfrom PIL import Image\n\nimport numpy as np\nimport csv\nimport os\n\nfrom tqdm import tqdm\nimport wandb\n\nimport gc\n\nfrom skimage.io import imshow\nimport matplotlib.pyplot as plt\n\nimport sys\n\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n#!g1.1\ndef empty_cache():\n    torch.cuda.empty_cache()\n    gc.collect()\n\nclass ImagesDataset(Dataset):\n    def __init__(self, img_dir, labels, transform=None):\n        self.samples = []\n\n        for label in labels:\n            img_path = os.path.join(img_dir, label[0])\n            self.samples.append((img_path, int(label[1])))\n\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, index):\n        img_path, label = self.samples[index]\n        img = Image.open(img_path).convert('RGB')\n\n        if self.transform:\n            img = self.transform(img)\n\n        return img, label\n    \n#!g1.1\n\nclass ImagesTestDataset(Dataset):\n    def __init__(self, img_dir, transform=None):\n        self.samples = []\n        \n        for filename in os.listdir(img_dir):\n            img_path = os.path.join(img_dir, filename)\n            self.samples.append((img_path, filename))\n\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, index):\n        img_path, filename = self.samples[index]\n        img = Image.open(img_path).convert('RGB')\n\n        if self.transform:\n            img = self.transform(img)\n\n        return img, filename\n    \ndef read_labels(labels_filename):\n    labels = None\n    with open(labels_filename, \"r\") as labels_file:\n        csvreader = csv.reader(labels_file)\n        labels = np.array([row for row in csvreader])[1:]\n    return labels    \n\n#!g1.1\ndef loaders(trainval_img_dir, labels_filename, train_transform, test_transform, batch_size=64, val_size=0.3):    \n    labels = read_labels(labels_filename)\n    \n    #torch 12\n    num_val_labels = int(val_size * len(labels))\n    train_idx, val_idx = random_split(np.arange(len(labels)), (len(labels) - num_val_labels, num_val_labels))\n    \n    # torch 13\n    #train_idx, val_idx = random_split(np.arange(len(labels)), (1 - val_size, val_size))\n\n    train_dataset = ImagesDataset(trainval_img_dir, labels[train_idx.indices], transform=train_transform)\n    val_dataset = ImagesDataset(trainval_img_dir, labels[val_idx.indices], transform=test_transform)\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    \n    return train_loader, val_loader\n\ndef subset_loaders(trainval_img_dir, labels_filename, train_transform, test_transform, batch_size=64, val_size=0.3, ratio=0.5):\n    labels = read_labels(labels_filename)\n    \n    #torch 12\n    num_val_labels = int(val_size * len(labels))\n    train_idx, val_idx = random_split(np.arange(len(labels)), (len(labels) - num_val_labels, num_val_labels))\n    \n    train_idx = torch.utils.data.Subset(train_idx, np.arange(int(len(train_idx) * ratio)))\n    val_idx = torch.utils.data.Subset(train_idx, np.arange(int(len(val_idx) * ratio)))\n    \n    # torch 13\n    #train_idx, val_idx = random_split(np.arange(len(labels)), (1 - val_size, val_size))\n\n    train_dataset = ImagesDataset(trainval_img_dir, labels[train_idx.indices], transform=train_transform)\n    val_dataset = ImagesDataset(trainval_img_dir, labels[val_idx.indices], transform=test_transform)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    return train_loader, val_loader\n\n#!g1.1\nclass SaveBestModel:\n    def __init__(self, best_val_loss=np.inf):\n        self.best_val_loss = best_val_loss\n        \n    def __call__(self, val_loss, epoch, model, optimizer, scheduler=None, model_path='model/best_model.pth'):\n        if val_loss < self.best_val_loss:\n            self.best_val_loss = val_loss\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict() if scheduler else {}\n                }, model_path\n            )\n            print('New best model with loss {:.5f} is saved'.format(val_loss))\n\ndef save_model(epoch, model, optimizer, scheduler=None, model_path='model/final_model.pth'):\n    torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict() if scheduler else {},\n                }, model_path\n    )\n    print('Model is saved')\n    \ndef load_model(model, optimizer, scheduler=None, model_path='model/best_model.pth'):\n    checkpoint = torch.load(model_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    epoch = checkpoint['epoch']\n    \n    if scheduler:\n        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n\n    return model, optimizer, epoch, scheduler\n\n#!g1.1\n\ndef train_epoch(model, optimizer, criterion, train_loader, device, tqdm_desc):\n    model.train()\n    train_acc, train_loss = 0.0, 0.0\n\n    for images, labels in tqdm(train_loader, desc=tqdm_desc):\n        images = images.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n        \n        logits = model(images)\n        loss = criterion(logits, labels)\n\n        loss.backward()\n        optimizer.step()\n\n        train_acc += (logits.argmax(dim=1) == labels).sum().item()\n        train_loss += loss.item() * labels.shape[0]\n\n    train_acc /= len(train_loader.dataset)\n    train_loss /= len(train_loader.dataset)\n\n    return train_acc, train_loss\n\n\n@torch.no_grad()\ndef val_epoch(model, criterion, val_loader, device, tqdm_desc):\n    model.eval()\n    val_acc, val_loss = 0.0, 0.0\n\n    for images, labels in tqdm(val_loader, desc=tqdm_desc):\n        images = images.to(device)\n        labels = labels.to(device)\n\n        logits = model(images)\n        loss = criterion(logits, labels)\n\n        val_acc += (logits.argmax(dim=1) == labels).sum().item()\n        val_loss += loss.item() * labels.shape[0]\n\n    val_acc /= len(val_loader.dataset)\n    val_loss /= len(val_loader.dataset)\n\n    return val_acc, val_loss\n\n\ndef train(model, optimizer, criterion, scheduler, train_loader, val_loader, device, num_epochs, model_saver, continue_training=True, model_path='model/best_model.pth', start_epoch=0):\n    \n    if continue_training:\n        model, optimizer, start_epoch, scheduler = load_model(model, optimizer, scheduler, model_path)\n    \n    for epoch in range(start_epoch + 1, num_epochs + 1):\n        train_acc, train_loss = train_epoch(model, optimizer, criterion, train_loader, device, f'Training epoch {epoch}/{num_epochs}')\n        val_acc, val_loss = val_epoch(model, criterion, val_loader, device, f'Validating epoch {epoch}/{num_epochs}')\n\n        if scheduler is not None:\n            scheduler.step()\n            #scheduler.step(val_loss)\n        \n        print({'epoch': epoch, 'train_loss': train_loss, 'train_acc': train_acc, 'val_loss': val_loss, 'val_acc': val_acc})\n        wandb.log({'train_loss': train_loss, 'train_acc': train_acc, 'val_loss': val_loss, 'val_acc': val_acc})\n        model_saver(val_loss, epoch, model, optimizer, scheduler, model_path)\n\ndef params_split_for_wd(model, verbose=False):\n    white_list_wd = []\n    black_list_wd = []\n    all_modules = (nn.Linear, nn.Conv2d, nn.BatchNorm2d, nn.BatchNorm1d)\n    \n    for module_name, module in model.named_modules():\n        if not isinstance(module, all_modules):\n            continue\n        for param_name, _ in module.named_parameters():\n            name = f\"{module_name}.{param_name}\" if module_name else param_name\n\n            if name.endswith(\"bias\"):\n                black_list_wd += [name]\n            elif name.endswith(\"weight\"):\n                if isinstance(module, (nn.Linear, nn.Conv2d)):\n                    white_list_wd += [name]\n                elif isinstance(module, (nn.BatchNorm2d, nn.BatchNorm1d)):\n                    black_list_wd += [name]\n    if verbose:\n        print(\"White list WD\")\n        print(white_list_wd)\n        print(\"Black list WD\")\n        print(black_list_wd)\n\n    model_params = {param_name: param for param_name, param in model.named_parameters()}\n    wd_params = [model_params[param] for param in white_list_wd]\n    no_wd_params = [model_params[param] for param in black_list_wd]\n    \n    return wd_params, no_wd_params\n\n#!g1.1\nclass TestNet(nn.Module):\n    \"\"\"\n        Model for checking that training pipeline is working\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        \n        self.fc = nn.Linear(3 * 224 * 224, 200)\n        \n    def forward(self, x):\n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n    \nclass Resnet18(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = resnet18(pretrained=False)\n        self.model.fc = nn.Linear(in_features=512, out_features=n_classes)\n        \n    def forward(self, x):\n        return self.model(x)\n    \nclass Resnet50(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = resnet50(pretrained=False)\n        self.model.fc = nn.Sequential(\n            nn.Linear(in_features=2048, out_features=1024),\n            nn.ReLU(),\n            nn.Linear(in_features=1024, out_features=512),\n            nn.ReLU(),\n            nn.Linear(in_features=512, out_features=256),\n            nn.ReLU(),\n            nn.Linear(in_features=256, out_features=n_classes)\n        ) \n        \n    def forward(self, x):\n        return self.model(x)\n    \nclass EnhancedResnet50(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = resnet50(pretrained=False)\n        self.model.fc = nn.Sequential(\n            nn.Linear(in_features=2048, out_features=1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Linear(in_features=1024, out_features=512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Linear(in_features=512, out_features=256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Linear(in_features=256, out_features=n_classes)\n        ) \n        \n    def forward(self, x):\n        return self.model(x)\n\ndef train_model():\n    model_saver = SaveBestModel()\n    \n    print(\"Get loaders...\")\n    train_loader, val_loader = loaders(trainval_img_dir, labels_filename, train_transform, test_transform, batch_size=batch_size, val_size=0.3)\n    #train_loader, val_loader = subset_loaders(trainval_img_dir, labels_filename, train_transform, test_transform, batch_size=batch_size, val_size=0.3, ratio=0.01)\n\n    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\n    wd_params, no_wd_params = params_split_for_wd(model, verbose=False)\n    optimizer = torch.optim.SGD([\n            {\"params\": wd_params, \"weight_decay\": 2e-05},\n            {\"params\": no_wd_params, \"weight_decay\": 0.0},\n        ], lr=1e-3, momentum=0.9)\n\n    #scheduler = None\n    #optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n\n    n_epochs = 100\n    lr_warmup_epochs = 5\n    lr_warmup_decay = 0.01\n\n    warmup_lr_scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=lr_warmup_decay, total_iters=lr_warmup_epochs)\n    main_lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs - lr_warmup_epochs, eta_min=0)\n\n    scheduler = optim.lr_scheduler.SequentialLR(\n        optimizer, schedulers=[warmup_lr_scheduler, main_lr_scheduler], milestones=[lr_warmup_epochs], verbose=True\n    )\n\n    #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=5)\n\n    print(\"Training start...\")\n    wandb.login(key=\"91898ab676432e8d5689a2ce4a88f7131dc1e45c\")\n    wandb.init(project=\"bhw1\")\n\n    os.mkdir('model')\n    train(model, optimizer, criterion, scheduler, train_loader, val_loader, device, n_epochs, model_saver, continue_training=False, model_path='model/best_model.pth')\n\n\n@torch.no_grad()\ndef predict(model, test_loader, device):\n    model.eval()\n    labels = [[\"Id\", \"Label\"]]\n    \n    for images, filenames in tqdm(test_loader, desc='Testing'):\n        images = images.to(device)\n        \n        logits = model(images)\n        preds = logits.argmax(dim=1)\n        \n        for filename, pred in zip(filenames, preds):\n            labels.append([filename, pred.item()])\n    \n    return labels\n\ndef test_model():\n    print(\"Get test loader...\")\n    test_dataset = ImagesTestDataset(test_img_dir, transform=test_transform)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n\n    print(\"Predicting...\")\n    labels = predict(model, test_loader, device)\n\n    print(\"Writing to file...\")\n    with open('labels_test.csv', 'w') as csvfile:\n        writer = csv.writer(csvfile)\n\n        for label in labels:\n            writer.writerow(label)\n    print(\"Finished!\")\n\nempty_cache()\n\ndata_dir = '../input/bhw-1-deep-learning/bhw1-dataset/'\ntrainval_img_dir = os.path.join(data_dir, 'trainval/')\ntest_img_dir = os.path.join(data_dir, 'test/')\nlabels_filename = os.path.join(data_dir, 'labels.csv')\n\n#!g1.1\nn_classes = 200\nbatch_size = 64\n\ntrain_transform = T.Compose([\n    T.RandomResizedCrop(224, scale=(0.1, 1.0)),\n    T.RandomHorizontalFlip(),\n    T.TrivialAugmentWide(),\n    #T.RandomGrayscale(p=0.1),\n    #T.RandomApply([T.RandomRotation(degrees=30)], p=0.5),\n    T.ToTensor(),\n    T.RandomErasing(p=0.1),\n    T.Normalize(mean=(0.5695764, 0.5449682, 0.4936079), std=(0.24523072, 0.2391582, 0.25806385)),\n])\n\ntest_transform = T.Compose([\n    T.Resize(256),\n    T.CenterCrop(224),\n    T.ToTensor(),\n    T.Normalize(mean=(0.5611811, 0.53794473, 0.48733008), std=(0.24465169, 0.23830907, 0.25577575))\n    #T.Normalize(mean=(0.5695764, 0.5449682, 0.4936079), std=(0.24523072, 0.2391582, 0.25806385))\n])\n\n#CUDA = 0 if len(sys.argv) == 1 else int(sys.argv[1])\nCUDA = 0\n\nprint(f'Trying working on device cuda:{CUDA}...')\ndevice = torch.device(f'cuda:{CUDA}' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\nprint(\"Load model...\")\nmodel = EnhancedResnet50().to(device)\nmodel.load_state_dict(torch.load('../input/model077/best_model4.pth', map_location=f'cuda:{CUDA}')['model_state_dict'])\ntrain_model()\n#model.model.fc = nn.Linear(in_features=2048, out_features=n_classes)\n#model = model.to(device)\n#model = EnhancedResnet50().to(device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-18T08:59:21.156552Z","iopub.execute_input":"2022-12-18T08:59:21.156948Z","iopub.status.idle":"2022-12-18T09:01:02.849452Z","shell.execute_reply.started":"2022-12-18T08:59:21.156910Z","shell.execute_reply":"2022-12-18T09:01:02.848465Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Trying working on device cuda:0...\ncuda:0\nLoad model...\nGet test loader...\nPredicting...\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 10000/10000 [01:40<00:00, 99.26it/s]","output_type":"stream"},{"name":"stdout","text":"Writing to file...\nFinished!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}